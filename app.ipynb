{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in ./.venv/lib/python3.12/site-packages (20240930)\n",
      "Requirement already satisfied: langchain-ollama in ./.venv/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: more-itertools in ./.venv/lib/python3.12/site-packages (from openai-whisper) (10.6.0)\n",
      "Requirement already satisfied: numba in ./.venv/lib/python3.12/site-packages (from openai-whisper) (0.61.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from openai-whisper) (2.1.3)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.12/site-packages (from openai-whisper) (0.8.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from openai-whisper) (2.5.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: triton>=2 in ./.venv/lib/python3.12/site-packages (from openai-whisper) (3.1.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in ./.venv/lib/python3.12/site-packages (from langchain-ollama) (0.3.31)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in ./.venv/lib/python3.12/site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.3.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in ./.venv/lib/python3.12/site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from triton>=2->openai-whisper) (3.17.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba->openai-whisper) (0.44.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.12/site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/Documentos/faculdade/projeto-transcricao-audio/.venv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O texto \"Que é isso, meu filho calma?\" pode ser resumido dos seguintes pontos-chave:\n",
      "\n",
      "1. Pergunta: A frase é uma pergunta feita por um pai para seu filho, que significa \"O que está acontecendo aqui?\" ou \"Que é este estranho phenomenon?\".\n",
      "2. Contexto: A pergunta pode ser usada em situações where something unusual or unexpected is happening, and the father is trying to make sense of it.\n",
      "3. Tone: The tone of the pergunta is calm and curious, as if the father is trying to gather information from his son.\n",
      "4. Language: The language used in the pergunta is simple and straightforward, with a touch of playfulness and affection.\n",
      "5. Purpose: The purpose of the pergunta is likely to elicit information or insight from the son, or to simply spend time together and enjoy each other's company.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "def transcribe_and_summarize(audio_file):\n",
    "    \"\"\"\n",
    "    Transcreve um arquivo de áudio e gera um resumo dos pontos-chave.\n",
    "\n",
    "    Args:\n",
    "        audio_file (str): Caminho para o arquivo de áudio.\n",
    "\n",
    "    Returns:\n",
    "        str: Resumo dos pontos-chave.\n",
    "    \"\"\"\n",
    "\n",
    "    # Carregar o modelo Whisper\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    # Transcrever o áudio\n",
    "    result = model.transcribe(audio_file)\n",
    "    text = result[\"text\"]\n",
    "\n",
    "    # Configurar o Ollama\n",
    "    client = OllamaLLM(model=\"llama2:latest\")\n",
    "    \n",
    "    # Gerar o resumo\n",
    "    prompt = f\"Resuma os pontos-chave do seguinte texto: {text}\"\n",
    "    response = client.invoke(prompt)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Exemplo de uso\n",
    "audio_file = \"audio.mp3\"\n",
    "summary = transcribe_and_summarize(audio_file)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
