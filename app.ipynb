{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai-whisper in ./.venv/lib/python3.12/site-packages (20240930)\n",
      "Requirement already satisfied: langchain-ollama in ./.venv/lib/python3.12/site-packages (0.2.2)\n",
      "Requirement already satisfied: more-itertools in ./.venv/lib/python3.12/site-packages (from openai-whisper) (10.6.0)\n",
      "Requirement already satisfied: numba in ./.venv/lib/python3.12/site-packages (from openai-whisper) (0.61.0)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.12/site-packages (from openai-whisper) (2.1.3)\n",
      "Requirement already satisfied: tiktoken in ./.venv/lib/python3.12/site-packages (from openai-whisper) (0.8.0)\n",
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (from openai-whisper) (2.5.1)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.12/site-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: triton>=2 in ./.venv/lib/python3.12/site-packages (from openai-whisper) (3.1.0)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in ./.venv/lib/python3.12/site-packages (from langchain-ollama) (0.3.31)\n",
      "Requirement already satisfied: ollama<1,>=0.4.4 in ./.venv/lib/python3.12/site-packages (from langchain-ollama) (0.4.7)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.3.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./.venv/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-ollama) (4.12.2)\n",
      "Requirement already satisfied: httpx<0.29,>=0.27 in ./.venv/lib/python3.12/site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from triton>=2->openai-whisper) (3.17.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in ./.venv/lib/python3.12/site-packages (from numba->openai-whisper) (0.44.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in ./.venv/lib/python3.12/site-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in ./.venv/lib/python3.12/site-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (3.1.5)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (12.4.127)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (75.8.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in ./.venv/lib/python3.12/site-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.8.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.12/site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in ./.venv/lib/python3.12/site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in ./.venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain-ollama) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.12/site-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/Documentos/faculdade/projeto-transcricao-audio/.venv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "O texto \"Olha que desgraçado de novo aqui!\" contém algumas informações importantes:\n",
      "\n",
      "1. Desgraça: O termo \"desgraçado\" é usado para descrever uma situação ou pessoa que está passando por um mal-estar ou desvantagem.\n",
      "2. Novamente: A palavra \"novamente\" indica que a situação de desgraça já foi enfrentada anteriormente.\n",
      "3. Aqui: O local onde a situação de desgraça ocorreu é mencionado.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "def transcribe_and_summarize(audio_file):\n",
    "    \"\"\"\n",
    "    Transcreve um arquivo de áudio e gera um resumo dos pontos-chave.\n",
    "\n",
    "    Args:\n",
    "        audio_file (str): Caminho para o arquivo de áudio.\n",
    "\n",
    "    Returns:\n",
    "        str: Resumo dos pontos-chave.\n",
    "    \"\"\"\n",
    "\n",
    "    # Carregar o modelo Whisper\n",
    "    model = whisper.load_model(\"base\")\n",
    "\n",
    "    # Transcrever o áudio\n",
    "    result = model.transcribe(audio_file)\n",
    "    text = result[\"text\"]\n",
    "\n",
    "    # Configurar o Ollama\n",
    "    client = OllamaLLM(model=\"llama2:latest\")\n",
    "    \n",
    "    # Gerar o resumo\n",
    "    prompt = f\"Resuma os pontos-chave do seguinte texto: {text}\"\n",
    "    response = client.invoke(prompt)\n",
    "\n",
    "    return response\n",
    "\n",
    "# Exemplo de uso\n",
    "audio_file = \"audio.mp3\"\n",
    "summary = transcribe_and_summarize(audio_file)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
